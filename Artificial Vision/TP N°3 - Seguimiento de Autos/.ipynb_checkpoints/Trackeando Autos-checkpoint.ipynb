{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2do Intento con Codigo de Internet\n",
    "\n",
    "El que use:\n",
    "https://www.learnopencv.com/multitracker-multiple-object-tracking-using-opencv-c-python/\n",
    "\n",
    "El que no use:\n",
    "https://www.pyimagesearch.com/2018/07/30/opencv-object-tracking/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create a Single Object Tracker\n",
    "A multi-object tracker is simply a collection of single object trackers. We start by defining a function that takes a tracker type as input and creates a tracker object. OpenCV has 8 different tracker types : BOOSTING, MIL, KCF,TLD, MEDIANFLOW, GOTURN, MOSSE, CSRT.\n",
    "\n",
    "If you want to use the GOTURN tracker, please make sure to read this post and download the caffe model.\n",
    "\n",
    "In the code below, given the name of the tracker class, we return the tracker object. This will be later used to populate the multi-tracker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import cv2\n",
    "from random import randint\n",
    "import os\n",
    "\n",
    "trackerTypes = ['BOOSTING', 'MIL', 'KCF','TLD', 'MEDIANFLOW', 'GOTURN', 'MOSSE', 'CSRT']\n",
    "\n",
    "def createTrackerByName(trackerType):\n",
    "# Create a tracker based on tracker name\n",
    "    if trackerType == trackerTypes[0]:\n",
    "        tracker = cv2.TrackerBoosting_create()\n",
    "    elif trackerType == trackerTypes[1]:\n",
    "        tracker = cv2.TrackerMIL_create()\n",
    "    elif trackerType == trackerTypes[2]:\n",
    "        tracker = cv2.TrackerKCF_create()\n",
    "    elif trackerType == trackerTypes[3]:\n",
    "        tracker = cv2.TrackerTLD_create()\n",
    "    elif trackerType == trackerTypes[4]:\n",
    "        tracker = cv2.TrackerMedianFlow_create()\n",
    "    elif trackerType == trackerTypes[5]:\n",
    "        tracker = cv2.TrackerGOTURN_create()\n",
    "    elif trackerType == trackerTypes[6]:\n",
    "        tracker = cv2.TrackerMOSSE_create()\n",
    "    elif trackerType == trackerTypes[7]:\n",
    "        tracker = cv2.TrackerCSRT_create()\n",
    "    else:\n",
    "        tracker = None\n",
    "    \n",
    "        print('Incorrect tracker name')\n",
    "        print('Available trackers are:')\n",
    "        \n",
    "        for t in trackerTypes:\n",
    "            print(t)\n",
    "\n",
    "    return tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Read First Frame of a Video\n",
    "A multi-object tracker requires two inputs\n",
    "\n",
    "A video frame\n",
    "Location (bounding boxes) of all objects we want to track.\n",
    "Given this information, the tracker tracks the location of these specified objects in all subsequent frames.\n",
    "\n",
    "In the code below, we first load the video using the VideoCapture class and read the first frame. This will be used later to initialize the MultiTracker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mostrar frames que me importan para identificar cuando aparece un auto nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# capture = cv2.VideoCapture(\"C:/Users/Pedro Vallarino/Desktop/Scripts/Python/Vision Artificial/TP N°3 - Seguimiento de Autos/carsRt9_3.avi\")\n",
    "# frame_count = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "# print('Frame count:', frame_count)\n",
    "\n",
    "# for x in range(0,50):\n",
    "\n",
    "#     capture.set(cv2.CAP_PROP_POS_FRAMES, x*10)\n",
    "#     print('Position:', int(capture.get(cv2.CAP_PROP_POS_FRAMES)))\n",
    "#     _, frame = capture.read()\n",
    "#     cv2.imshow('frame', frame)\n",
    "#     cv2.waitKey(0)\n",
    "\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Video\n"
     ]
    }
   ],
   "source": [
    "# Set video to load\n",
    "videoPath = \"C:/Users/Pedro Vallarino/Desktop/Scripts/Python/Vision Artificial/TP N°3 - Seguimiento de Autos/carsRt9_3.avi\"\n",
    "\n",
    "# Create a video capture object to read videos\n",
    "cap = cv2.VideoCapture(videoPath)\n",
    "\n",
    "# Read first frame\n",
    "success, frame = cap.read()\n",
    "# quit if unable to read the video file\n",
    "if not success:\n",
    "    print('Failed to read video')\n",
    "    sys.exit(1)\n",
    "else:\n",
    "    print('Found Video')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Locate Objects in the First Frame\n",
    "Next, we need to locate objects we want to track in the first frame. The location is simply a bounding box.\n",
    "\n",
    "OpenCV provides a function called selectROI that pops up a GUI to select bounding boxes (also called a Region of Interest (ROI)).\n",
    "\n",
    "In the C++ version, selectROI allows you to obtain multiple bounding boxes, but in the Python version, it returns just one bounding box. So, in the Python version, we need a loop to obtain multiple bounding boxes.\n",
    "\n",
    "For every object, we also select a random color to display the bounding box.\n",
    "\n",
    "Proceso:\n",
    "- Seleccionar ROI\n",
    "- Apretar doble Enter\n",
    "- Seleccionar otro ROI \n",
    "- Repetir el proceso hasta que haya seleccionado todos los que quiero\n",
    "- Una vez seleccionado todos ROI apretar un solo enter y luego apretar dos veces q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press q to quit selecting boxes and start tracking\n",
      "Press n to skip frame and keep deciding RoI\n",
      "Press s to select next object and then enter to execute\n",
      "Once inside press e to escape\n",
      "Press Enter to continue...s\n",
      "s\n",
      "Press q to quit selecting boxes and start tracking\n",
      "Press n to skip frame and keep deciding RoI\n",
      "Press s to select next object and then enter to execute\n",
      "Once inside press e to escape\n",
      "Press Enter to continue...q\n",
      "q\n",
      "Selected bounding boxes [(439, 162, 46, 14)]\n"
     ]
    }
   ],
   "source": [
    "## Select boxes\n",
    "bboxes = []\n",
    "colors = [] \n",
    "i = 1\n",
    "\n",
    "# OpenCV's selectROI function doesn't work for selecting multiple objects in Python\n",
    "# So we will call this function in a loop till we are done selecting all objects\n",
    "# for x in range(0,len(frame))\n",
    "while True:\n",
    "  # draw bounding boxes over objects\n",
    "  # selectROI's default behaviour is to draw box starting from the center\n",
    "  # when fromCenter is set to false, you can draw box starting from top left corner\n",
    "    \n",
    "    \n",
    "    print(\"Press q to quit selecting boxes and start tracking\")\n",
    "    print(\"Press n to skip frame and keep deciding RoI\")\n",
    "    print(\"Press s to select next object and then enter to execute\")\n",
    "    print(\"Once inside press e to escape\")\n",
    "   \n",
    "    k = input(\"Press Enter to continue...\")\n",
    "    print(k)\n",
    "    \n",
    "    \n",
    "    if k == 'q' :  # q is pressed\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "    \n",
    "    if k == 'n' :  # n is pressed\n",
    "        while True:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "            _, frame = cap.read()\n",
    "            i += 1\n",
    "            cv2.imshow('frame', frame)\n",
    "            t = cv2.waitKey(0) & 0xFF\n",
    "            cv2.destroyAllWindows()\n",
    "            if t == ord('e'):\n",
    "                break\n",
    "                \n",
    "        continue\n",
    "    \n",
    "    if k == 's' :  # s is pressed\n",
    "        while True:\n",
    "            bbox = cv2.selectROI('MultiTracker', frame)\n",
    "            bboxes.append(bbox)\n",
    "            colors.append((randint(0, 255), randint(0, 255), randint(0, 255)))       \n",
    "            t = cv2.waitKey(0) & 0xFF\n",
    "            if t == ord('e'):\n",
    "                break\n",
    "                \n",
    "        continue\n",
    "    \n",
    "\n",
    "print('Selected bounding boxes {}'.format(bboxes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Initialize the MultiTracker\n",
    "Until now, we have read the first frame and obtained bounding boxes around objects. That is all the information we need to initialize the multi-object tracker.\n",
    "\n",
    "We first create a MultiTracker object and add as many single object trackers to it as we have bounding boxes. In this example, we use the CSRT single object tracker, but you try other tracker types by changing the trackerType variable below to one of the 8 tracker times mentioned at the beginning of this post. The CSRT tracker is not the fastest but it produces the best results in many cases we tried.\n",
    "\n",
    "You can also use different trackers wrapped inside the same MultiTracker, but of course, it makes little sense.\n",
    "\n",
    "The MultiTracker class is simply a wrapper for these single object trackers. As we know from our previous post, the single object tracker is initialized using the first frame and the bounding box indicating the location of the object we want to the track. The MultiTracker passes this information over to the single object trackers it is wrapping internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the tracker type\n",
    "trackerType = \"CSRT\"    \n",
    "\n",
    "# Create MultiTracker object\n",
    "multiTracker = cv2.MultiTracker_create()\n",
    "\n",
    "# Initialize MultiTracker \n",
    "for bbox in bboxes:\n",
    "    multiTracker.add(createTrackerByName(trackerType), frame, bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Update MultiTracker & Display Results\n",
    "Finally, our MultiTracker is ready and we can track multiple objects in a new frame. We use the update method of the MultiTracker class to locate the objects in a new frame. Each bounding box for each tracked object is drawn using a different color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_img = 1\n",
    "path = r'C:/Users/Pedro Vallarino/Desktop/Scripts/Python/Vision Artificial/TP N°3 - Seguimiento de Autos/images/'\n",
    "\n",
    "# Process video and track objects\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "\n",
    "    # get updated location of objects in subsequent frames\n",
    "    success, boxes = multiTracker.update(frame)\n",
    "\n",
    "    # draw tracked objects\n",
    "    for i, newbox in enumerate(boxes):\n",
    "        p1 = (int(newbox[0]), int(newbox[1]))\n",
    "        p2 = (int(newbox[0] + newbox[2]), int(newbox[1] + newbox[3]))\n",
    "        cv2.rectangle(frame, p1, p2, colors[i], 2, 1)\n",
    "\n",
    "    # show frame\n",
    "    cv2.imshow('MultiTracker', frame)\n",
    "#     cv2.imwrite('kang'+str(num_img)+'.jpg',frame)\n",
    "#     num_img = num_img + 1\n",
    "    \n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "    if k == 'q' :  # q is pressed\n",
    "        cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3er Intento\n",
    "\n",
    "Es distinto, nose si usarlo porque labura con modelos entrenados y con machine learning y no me parece que el trabajo apunta a esto\n",
    "\n",
    "https://heartbeat.fritz.ai/detecting-objects-in-videos-and-camera-feeds-using-keras-opencv-and-imageai-c869fe1ebcdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "# !pip install keras\n",
    "# !pip install imageai --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4to Intento\n",
    "https://www.youtube.com/watch?v=MkcUgPhOlP8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture('carsRt9_3.avi')\n",
    "\n",
    "ret,frame1 = cap.read()\n",
    "ret,frame2 = cap.read()\n",
    "kernel_D = np.ones((2,1), np.uint8) \n",
    "kernel_E1 = np.ones((2,2), np.uint8) \n",
    "kernel_E2 = np.ones((1,1), np.uint8) \n",
    "\n",
    "while cap.isOpened:\n",
    "    try:\n",
    "        diff = cv2.absdiff(frame1,frame2)\n",
    "    except:\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "    \n",
    "    gray = cv2.cvtColor(diff,cv2.COLOR_BGR2GRAY)    \n",
    "    blur = cv2.GaussianBlur(gray, ksize = (5,5), sigmaX = 0.3)\n",
    "    _, thresh = cv2.threshold(blur,20,255,cv2.THRESH_BINARY)\n",
    "#     erosion = cv2.erode(thresh, kernel = None, iterations=1) \n",
    "    \n",
    "    dilated = cv2.dilate(thresh, kernel_D , iterations = 2)\n",
    "    \n",
    "    contours1, _ = cv2.findContours(dilated[0:180],cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours2, _ = cv2.findContours(dilated[180:400],cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    \n",
    "#     cv2.drawContours(frame1, contours, contourIdx = -1, color = (0,255,0), thickness = 1)\n",
    "    \n",
    "    for contour1 in contours1:\n",
    "        (x1,y1,w1,h1) = cv2.boundingRect(contour)\n",
    "        \n",
    "        if cv2.contourArea(contour1) < 100:\n",
    "            continue\n",
    "        \n",
    "        cv2.rectangle(frame1[0:180], (x1,y1), (x1+w1,y1+h1), (0,255,0),2)    \n",
    "    \n",
    "            \n",
    "    for contour2 in contours2:\n",
    "        (x2,y2,w2,h2) = cv2.boundingRect(contour2)\n",
    "        \n",
    "        if cv2.contourArea(contour2) < 200:\n",
    "            continue\n",
    "                    \n",
    "        cv2.rectangle(frame1[180:400], (x2,y2), (x2+w2,y2+h2), (255,0,0),2)    \n",
    "        \n",
    "    \n",
    "    cv2.imshow(\"thresh\",thresh)\n",
    "    cv2.moveWindow(\"thresh\", 0,0)\n",
    "            \n",
    "    cv2.imshow(\"dilated\",dilated)\n",
    "    cv2.moveWindow(\"dilated\", 890,0)\n",
    "    \n",
    "#     cv2.imshow(\"erosion\",erosion)\n",
    "#     cv2.moveWindow(\"erosion\", 890,420)\n",
    "    \n",
    "    cv2.imshow(\"frame1\",frame1)\n",
    "    cv2.moveWindow(\"frame1\", 0,420)\n",
    "    \n",
    "    \n",
    "    k = cv2.waitKey(50)\n",
    "    \n",
    "    if k == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "    \n",
    "    if k == ord('s'):\n",
    "        cv2.waitKey(0)\n",
    "        continue\n",
    "    \n",
    "    frame1 = frame2\n",
    "    ret, frame2 = cap.read()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicacion de que esta pasando y como mejorarlo.\n",
    "- Problemas\n",
    "    - Lo que esta pasando cuando pasa adelante de objetos es que la imagen binaria se parte en dos y no dibuja los dos rectangulos porque hay un filtro de area de rectangulos para eliminar los rectangulos de ruido\n",
    "    - Bajar el threshold mejora la definicion de la imagen lo que evitaria que se parta pero no sirve porque aumenta el ruido trayendo entonces los rectangulos indeseados\n",
    "- Soluciones\n",
    "    - Pedir ayuda a tincho con la mascara para sacar el ruido de fondo o probar erosionar antes de dilatar la imagen binaria para eliminar el ruido entonces\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5to Intento\n",
    "Tincho Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "height = []\n",
    "\n",
    "cap = cv2.VideoCapture('carsRt9_3.avi')\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2(history=200, varThreshold = 25,detectShadows = True)\n",
    "kernel_E1 = np.ones((2,2), np.uint8) \n",
    "kernel_D = np.ones((1,2), np.uint8) \n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        ret,frame1 = cap.read()\n",
    "        fgmask = fgbg.apply(frame1)\n",
    "        _, thresh = cv2.threshold(fgmask,5,255,cv2.THRESH_BINARY)\n",
    "#         erosion = cv2.erode(thresh, kernel_E1, iterations=1) \n",
    "        dilated = cv2.dilate(thresh, kernel_D , iterations = 2)\n",
    "        \n",
    "        contours, _ = cv2.findContours(dilated[140:183],cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        for contour in contours:\n",
    "            (x,y,w,h) = cv2.boundingRect(contour)\n",
    "            \n",
    "            height.append(h)\n",
    "            \n",
    "            if cv2.contourArea(contour) < 300:\n",
    "                continue\n",
    "\n",
    "            # Metodo de altura (h)\n",
    "                \n",
    "            if 0 < h < 19:\n",
    "                cv2.rectangle(frame1[140:183], (x,y), (x+w,y+h), (0,255,0),2)   \n",
    "            \n",
    "            if 19 < h < 25:\n",
    "                cv2.rectangle(frame1[140:183], (x,y), (x+w,y+h), (0,0,255),2)   \n",
    "\n",
    "            if 25 < h :\n",
    "                cv2.rectangle(frame1[140:183], (x,y), (x+w,y+h), (255,0,0),2)   \n",
    "                \n",
    "                \n",
    "        contours, _ = cv2.findContours(dilated[183:400],cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        for contour in contours:\n",
    "            (x,y,w,h) = cv2.boundingRect(contour)\n",
    "            \n",
    "            height.append(h)\n",
    "\n",
    "            if cv2.contourArea(contour) < 250:\n",
    "                continue\n",
    "\n",
    "\n",
    "            # Metodo de altura (h)\n",
    "                \n",
    "            if 0 < h < 25:\n",
    "                cv2.rectangle(frame1[183:400], (x,y), (x+w,y+h), (0,255,0),2)   \n",
    "                \n",
    "\n",
    "            if 25 < h :\n",
    "                cv2.rectangle(frame1[183:400], (x,y), (x+w,y+h), (0,0,255),2)   \n",
    "\n",
    "        \n",
    "        cv2.putText(fgmask, '1ro - BackGround Remover', (10,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,255), 2)\n",
    "        cv2.putText(thresh, '2do - Threshold Aplicado', (10,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,255), 2)\n",
    "        cv2.putText(dilated, '3ro - Dialtacion', (10,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,255), 2)\n",
    "        cv2.putText(frame1, '4to - Contornos en Realidad', (10,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,255), 2)\n",
    "                \n",
    "#         cv2.imshow('fgmask',fgmask)\n",
    "#         cv2.imshow('thresh',thresh)\n",
    "#         cv2.imshow('dilated',dilated)\n",
    "        cv2.imshow('frame1',frame1)\n",
    "        \n",
    "        cv2.moveWindow(\"fgmask\", 0,0)\n",
    "        cv2.moveWindow(\"thresh\", 890,0)\n",
    "        cv2.moveWindow(\"dilated\", 0,420)\n",
    "        cv2.moveWindow(\"frame1\", 890,420)\n",
    "        \n",
    "        k = cv2.waitKey(50)\n",
    "        \n",
    "        if k == ord('q'):\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "    \n",
    "        if k == ord('s'):\n",
    "            cv2.waitKey(0)\n",
    "            continue\n",
    "        \n",
    "    \n",
    "    except:        \n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()    \n",
    "\n",
    "#El GRAN PROBLEMA que todavia tiene es el tema de las sombras, voy a tratar de procesar el video de antemano para sacarle las sombrass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imshow('a',frame1)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.imwrite('frame.jpg',frame1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected Ptr<cv::UMat> for argument 'img'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-62a8ffec669c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Crop the center of the image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mim2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mim1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m75\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m150\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'framecrop.jpg'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mim2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: Expected Ptr<cv::UMat> for argument 'img'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "im1 = Image.open('frame.jpg')\n",
    "width, height = im.size   # Get dimensions\n",
    "\n",
    "# print((x,y), (x+w,y+h))\n",
    "# print(width,height)\n",
    "\n",
    "# print(left,top,right,bottom)\n",
    "\n",
    "# Crop the center of the image\n",
    "im2 = im1.crop((100, 75, 300, 150))\n",
    "cv2.imwrite('framecrop.jpg',im2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
